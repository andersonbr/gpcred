Nesse trabalho, encaramos o fato que nem todos os exemplos de treinamento devem contribuir igualmente para a construção do modelo de classificação, e, portanto, considerar que alguns exemplos são mais confiáveis que outros pode aumentar a eficácia do classificador. 
Para isso, propomos o emprego de \textbf{funções de credibilidade} que são capazes de capturar o quanto um classificador pode confiar em um exemplo ao gerar o modelo.

A credibilidade é considerada na literatura como dependente do contexto no qual está inserida, além de ser também dependente de quem a estima.
Para tornar mais objetiva sua avaliação, recomenda-se que sejam definidos os fatores que influenciam no seu cálculo.
Definimos que, do ponto de vista de um classificador, dois fatores cruciais são os atributos e relacionamentos apresentados pelos exemplos.

Construímos 30 métricas para explorar a credibilidade dos atributos e 16 para os relacionamentos. Elas foram inspiradas em métricas presentes na literatura que indicam a separação entre as classes e investigam as características dos relacionamentos entre os exemplos. Pelo fato de possuirmos um grande número de métricas para cada fator, empregamos o uso da Programação Genética, um mecanismo baseado no princípio de evolução de Darwin, capaz de percorrer, de forma robusta e eficaz, o grande espaço de busca criado.

Realizamos diversos experimentos com três tipos de bases de dados: bases de documentos, bases da \textsc{UCI} com atributos exclusivamente categóricos e uma grande base de assinaturas proteicas. Os resultados mostram ganhos consideráveis em todos os cenários, culminando em melhorias de até 17.51\% na Macro$F_1$ da base \textit{Ohsumed} e de 26.58\% e 50.78\% na Micro$F_1$ e Macro$F_1$ da base de assinaturas estruturais proteicas.

\keywords{Classificação automática, Programação Genética, Credibilidade}
